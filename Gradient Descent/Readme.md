Gradient descent
```
1.Calculate the hypothesis h = X * beta
2.Calculate the loss = h - y and maybe the squared cost (loss^2)/2m
3.Calculate the gradient = X' * loss / m
4.Update the parameters beta = beta - alpha * gradient
```

Batch vs stochastiic : **loss** from above for whole training set- batch , for each sample - stochastic

Batch:
http://www.bogotobogo.com/python/python_numpy_batch_gradient_descent_algorithm.php

Stochastic:
https://machinelearningmastery.com/linear-regression-tutorial-using-gradient-descent-for-machine-learning/

http://www.bogotobogo.com/python/scikit-learn/Artificial-Neural-Network-ANN-1-Introduction.php
